# Description: Experiment Configuration
---
version: 1
id: "${EXP_NAME}"
repetitions: 1
# Metadata accepts any key-value that we want to register together with the experiment run.
metadata:
  system: spark
  system_version: 3.3.1
  table_format: iceberg
  table_format_version: 1.1.0
  scale_factor: "${EXP_SCALE_FACTOR}"
  mode: mor
  machine: "${EXP_MACHINE}"
  cluster_size: "${EXP_CLUSTER_SIZE}"
# The following parameter values will be used to replace the variables in the workload statements.
parameter_values:
  external_catalog: spark_catalog
  external_database: "external_tpcds_sf_${EXP_SCALE_FACTOR}"
  external_table_format: csv
  external_data_path: "abfss://azure-pipelines@lstbenchdatasets.dfs.core.windows.net/tpc-ds/csv/sf_${EXP_SCALE_FACTOR}/"
  external_options_suffix: ',header="true"'
  external_tblproperties_suffix: ''
  catalog: spark_catalog
  database: "${EXP_NAME}"
  table_format: iceberg
  data_path: 'abfss://azure-pipelines@lstbenchdatasets.dfs.core.windows.net/tpc-ds/run/iceberg/sf_${EXP_SCALE_FACTOR}/'
  options_suffix: ''
  tblproperties_suffix: ', "format-version"="2", "write.delete.mode"="merge-on-read", "write.update.mode"="merge-on-read", "write.merge.mode"="merge-on-read"'
